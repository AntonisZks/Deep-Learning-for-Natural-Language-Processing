{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d8f691",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1><b>Feed Forward Neural Networks for Natural Language Processing<b></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f6453",
   "metadata": {},
   "source": [
    "### Download GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9499e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148388, 3) (42396, 3) (21199, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset_base_filepath = '../Data/Raw' # \"/kaggle/input/ai-2-deep-learning-for-nlp-homework-1\"\n",
    "images_base_filepath = '../imgs' # \"/kaggle/input/images\"\n",
    "\n",
    "# Load all the dataset files using pandas and store inside some dataframe variables\n",
    "train_df = pd.read_csv(f'{dataset_base_filepath}/train_dataset.csv')\n",
    "val_df = pd.read_csv(f'{dataset_base_filepath}/val_dataset.csv')\n",
    "test_df = pd.read_csv(f'{dataset_base_filepath}/test_dataset.csv')\n",
    "\n",
    "# Reduce the data sizes\n",
    "size=1\n",
    "train_df = train_df.sample(frac=size)\n",
    "val_df = val_df.sample(frac=size)\n",
    "# test_df = test_df.sample(frac=size)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0421bb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34518</th>\n",
       "      <td>67309</td>\n",
       "      <td>@suicidalcats That's cool. I've never been the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143195</th>\n",
       "      <td>90927</td>\n",
       "      <td>No, I'm not coming on to you. I am quoting Ela...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106657</th>\n",
       "      <td>196113</td>\n",
       "      <td>@musicalverse Ah well, at least we both got th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97983</th>\n",
       "      <td>83600</td>\n",
       "      <td>Morning!! Todays the game, and the Amount Boyz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114890</th>\n",
       "      <td>138451</td>\n",
       "      <td>@mot_mot lol no prob in that...so do i. especi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               Text  Label\n",
       "34518    67309  @suicidalcats That's cool. I've never been the...      1\n",
       "143195   90927  No, I'm not coming on to you. I am quoting Ela...      0\n",
       "106657  196113  @musicalverse Ah well, at least we both got th...      1\n",
       "97983    83600  Morning!! Todays the game, and the Amount Boyz...      1\n",
       "114890  138451  @mot_mot lol no prob in that...so do i. especi...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99491d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Antonis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Antonis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import contractions\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# !unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = contractions.fix(text) # Expand contractions\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text) # Remove url links\n",
    "    text = re.sub(r\"@\\w+\", \"\", text) # Remove mentions\n",
    "    text = re.sub(r\"#(\\w+)\", \"\", text) # Remove hastags\n",
    "    text = re.sub(r\"(?<!\\.)\\.(?!\\.)|[^\\w\\s\\?\\!]\", \"\", text)\n",
    "    text = re.sub(r\"\\.\\.\\.\", \" ... \", text)  # Ensure '...' is treated as a single token\n",
    "    \n",
    "    text = re.sub(r\"&[^;\\s]+;\", \"\", text) # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Remove extra spaces\n",
    "    \n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words] # Apply stemming\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words] # Apply lemmatization\n",
    "    text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d70da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets were cleaned in 46.984538555145264 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34518</th>\n",
       "      <td>67309</td>\n",
       "      <td>@suicidalcats That's cool. I've never been the...</td>\n",
       "      <td>1</td>\n",
       "      <td>that is cool i have never been there though _ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143195</th>\n",
       "      <td>90927</td>\n",
       "      <td>No, I'm not coming on to you. I am quoting Ela...</td>\n",
       "      <td>0</td>\n",
       "      <td>no i am not come on to you i am quot elain fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106657</th>\n",
       "      <td>196113</td>\n",
       "      <td>@musicalverse Ah well, at least we both got th...</td>\n",
       "      <td>1</td>\n",
       "      <td>ah well at least we both got the same wrong an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97983</th>\n",
       "      <td>83600</td>\n",
       "      <td>Morning!! Todays the game, and the Amount Boyz...</td>\n",
       "      <td>1</td>\n",
       "      <td>morning!! today the game and the amount boyz p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114890</th>\n",
       "      <td>138451</td>\n",
       "      <td>@mot_mot lol no prob in that...so do i. especi...</td>\n",
       "      <td>0</td>\n",
       "      <td>lol no prob in thatso do i especi now that my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87958</th>\n",
       "      <td>109124</td>\n",
       "      <td>Good Morning Twitterland! Heading off to the g...</td>\n",
       "      <td>0</td>\n",
       "      <td>good morn twitterland! head off to the gym hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63081</th>\n",
       "      <td>200481</td>\n",
       "      <td>@JayEv3ryDay Awww... and I miss him  lol</td>\n",
       "      <td>0</td>\n",
       "      <td>a and i miss him lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40214</th>\n",
       "      <td>25025</td>\n",
       "      <td>Watching Atonement on HBO. It's so sad.  Keira...</td>\n",
       "      <td>0</td>\n",
       "      <td>watch aton on hbo it is so sad keira knightli ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108059</th>\n",
       "      <td>142794</td>\n",
       "      <td>damn. I lost my @mordecai account to someone e...</td>\n",
       "      <td>0</td>\n",
       "      <td>damn i lost my account to someon el that wa fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50540</th>\n",
       "      <td>9413</td>\n",
       "      <td>@TIBlockhead this is so great...i think it's g...</td>\n",
       "      <td>0</td>\n",
       "      <td>thi is so greati think it is go to be a phenom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               Text  Label  \\\n",
       "34518    67309  @suicidalcats That's cool. I've never been the...      1   \n",
       "143195   90927  No, I'm not coming on to you. I am quoting Ela...      0   \n",
       "106657  196113  @musicalverse Ah well, at least we both got th...      1   \n",
       "97983    83600  Morning!! Todays the game, and the Amount Boyz...      1   \n",
       "114890  138451  @mot_mot lol no prob in that...so do i. especi...      0   \n",
       "87958   109124  Good Morning Twitterland! Heading off to the g...      0   \n",
       "63081   200481           @JayEv3ryDay Awww... and I miss him  lol      0   \n",
       "40214    25025  Watching Atonement on HBO. It's so sad.  Keira...      0   \n",
       "108059  142794  damn. I lost my @mordecai account to someone e...      0   \n",
       "50540     9413  @TIBlockhead this is so great...i think it's g...      0   \n",
       "\n",
       "                                             Cleaned_text  \n",
       "34518   that is cool i have never been there though _ ...  \n",
       "143195  no i am not come on to you i am quot elain fro...  \n",
       "106657  ah well at least we both got the same wrong an...  \n",
       "97983   morning!! today the game and the amount boyz p...  \n",
       "114890  lol no prob in thatso do i especi now that my ...  \n",
       "87958   good morn twitterland! head off to the gym hav...  \n",
       "63081                                a and i miss him lol  \n",
       "40214   watch aton on hbo it is so sad keira knightli ...  \n",
       "108059   damn i lost my account to someon el that wa fast  \n",
       "50540   thi is so greati think it is go to be a phenom...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Apply the cleaning function to every dataset file and create a new column with the modified text\n",
    "start_time = time.time()\n",
    "train_df[\"Cleaned_text\"] = train_df[\"Text\"].apply(clean_text)\n",
    "val_df[\"Cleaned_text\"] = val_df[\"Text\"].apply(clean_text)\n",
    "test_df[\"Cleaned_text\"] = test_df[\"Text\"].apply(clean_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Datasets were cleaned in {end_time - start_time} seconds.\")\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528aee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords were removed in 18.365330934524536 seconds.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"nltk\")\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    # Remove the stopwords from the original text\n",
    "    stop_words = set(\n",
    "        [\"i\", \"to\", \"the\", \"is\", \"a\", \"you\", \"my\", \"and\", \n",
    "         \"it\", \"am\", \"for\", \"in\", \"of\", \"that\", \"on\", \"so\", \"me\"]\n",
    "    )\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_words = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "start_time = time.time()\n",
    "train_df['Cleaned_text'] = train_df['Cleaned_text'].apply(remove_stopwords)\n",
    "val_df['Cleaned_text'] = val_df['Cleaned_text'].apply(remove_stopwords)\n",
    "test_df['Cleaned_text'] = test_df['Cleaned_text'].apply(remove_stopwords)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "print(f\"Stopwords were removed in {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13674f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
